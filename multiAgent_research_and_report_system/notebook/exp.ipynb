{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eae1546",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0d0837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_root():\n",
    "    return \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3fe4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c7cfa03",
   "metadata": {},
   "source": [
    "Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "215aed08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging' from 'c:\\\\Users\\\\muham\\\\Desktop\\\\Assignments\\\\common-venv\\\\lib\\\\logging\\\\__init__.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "\n",
    "# Setting log directory\n",
    "LOG_FILE = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M')}.log\"\n",
    "logs_dir = os.path.join(get_project_root(), \"logs\")\n",
    "\n",
    "# Creating log directory\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Configuring full log path \n",
    "LOGS_FILE_PATH = os.path.join(logs_dir, LOG_FILE)\n",
    "\n",
    "# Rotating Handler: 5MB max size, keep \n",
    "# handler = RotatingFileHandler(\"mylog.log\", maxBytes=5*1024*1024, backupCount=3)\n",
    "\n",
    "# Clear existing handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configuring logging\n",
    "logging.basicConfig(\n",
    "    filename=LOGS_FILE_PATH,\n",
    "    format=\"%(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# # Testing if logging working\n",
    "# if __name__ == \"__main__\":\n",
    "logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c882570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger_notebook import logging\n",
    "logging.info(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7b067",
   "metadata": {},
   "source": [
    "Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af6d8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Callable, Any\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_json(load_file:str):\n",
    "    \"\"\"\n",
    "    To load JSON files\n",
    "    \"\"\"\n",
    "\n",
    "    #at last i have to solve this directory by getting whole directory locatn\n",
    "    # Adding .json format if not available \n",
    "    if load_file[-5:] != \".json\":\n",
    "        load_file += \".json\"\n",
    "    file_path = os.path.join(get_project_root(), \"data\", load_file)\n",
    "\n",
    "    # Checking if file not exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: {load_file} not found!\")\n",
    "        return False\n",
    "    \n",
    "    # Loading pickle file\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file   :\n",
    "            json_file = json.load(file)\n",
    "        logging.info(f\"{load_file} loaded\")\n",
    "        print(f\"{load_file} loaded.\")\n",
    "        return json_file\n",
    "    except Exception as e:\n",
    "        print(\"Error while loading pickle file : \",e)\n",
    "\n",
    "    return json_file\n",
    "\n",
    "def save_as_json(save_obj:str, file_name:str):\n",
    "    \"\"\"\n",
    "    To save object as json file\n",
    "    \"\"\"\n",
    "\n",
    "    # Adding .json format if not available\n",
    "    if file_name[-5:] != \".json\":\n",
    "        file_name += \".json\"\n",
    "    file_path = os.path.join(get_project_root(), \"data\", file_name)\n",
    "    \n",
    "    # Creating path if not available and if file exists , returns\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Warning: {file_path} already exists \")\n",
    "        return False\n",
    "    \n",
    "    # Saving json file\n",
    "    try:\n",
    "        with open(file_path, 'w') as file: \n",
    "            json.dump(save_obj, file, indent=4)\n",
    "        logging.info(f\"{file_path} saved locally\")\n",
    "        print(f\"{file_path} saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving : {e}\")\n",
    "    return True\n",
    "\n",
    "def reload_json(json_file: str, func: Callable[[str], any], *args, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Loading json file\n",
    "    load_obj = load_json(json_file)\n",
    "    # If there is no json file , creating it by running function\n",
    "    if not load_obj:\n",
    "        if json_file[-5:] != \".json\":\n",
    "            json_file += \".json\"\n",
    "        print(f\"{json_file} file not exists. \\nCreating...\")\n",
    "        save_obj = func(*args, **kwargs)\n",
    "        if save_obj:\n",
    "            save_as_json(save_obj, json_file)\n",
    "        else:\n",
    "            print(\"Saving error : nothing returned by function {func.__name__}\")\n",
    "        return save_obj\n",
    "    return load_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d40d8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_llm(json_file: str, llm, message):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Loading json file\n",
    "    if json_file[-5:] != \".json\":\n",
    "            json_file += \".json\"\n",
    "\n",
    "    load_message = load_json(json_file)\n",
    "\n",
    "    # If there is no json file , creating it by running function\n",
    "    if not load_message:\n",
    "        print(f\"{json_file} file not exists. \\nCreating...\")\n",
    "        save_message = llm.invoke(message)\n",
    "        if save_message:\n",
    "            save_as_json(save_message, json_file)\n",
    "        else:\n",
    "            print(f\"Saving error : nothing returned by llm -> {llm}\")\n",
    "        return save_message\n",
    "    return load_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca2c4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(save_obj:str, file_name:str):\n",
    "    \"\"\"\n",
    "    To save object as pickle file\n",
    "    \"\"\"\n",
    "\n",
    "    # Adding .pkl format if not available\n",
    "    if file_name[-4:] != \".pkl\":\n",
    "        file_name += \".pkl\"\n",
    "    file_path = os.path.join(get_project_root(), \"data\", \"pickle_files\", file_name)\n",
    "    # file_path = f\"../data/pickle_files/{file_name}\"\n",
    "\n",
    "    # Creating path if not available and if file exists , returns\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Warning: {file_path} already exists \")\n",
    "        return False\n",
    "    \n",
    "    # Saving pickle file\n",
    "    try:\n",
    "        with open(file_path, 'wb') as file: \n",
    "            pickle.dump(save_obj, file)\n",
    "        logging.info(f\"{file_name} saved locally\")\n",
    "        print(f\"{file_name} saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving : {e}\")\n",
    "    return True\n",
    "\n",
    "def load_pickle(load_file:str):\n",
    "    \"\"\"\n",
    "    To load pickle objects\n",
    "    \"\"\"\n",
    "\n",
    "    # Adding .pkl format if not available \n",
    "    if load_file[-4:] != \".pkl\":\n",
    "        load_file += \".pkl\"\n",
    "    file_path = os.path.join(get_project_root(), \"data\", \"pickle_files\", load_file)\n",
    "    # file_path = f\"../data/pickle_files/{load_file}\"\n",
    "\n",
    "    # Checking if file not exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: {load_file} not found!\")\n",
    "        return False\n",
    "    \n",
    "    # Loading pickle file\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            pickle_file = pickle.load(f)\n",
    "        logging.info(f\"{load_file} loaded\")\n",
    "        print(f\"{load_file} loaded.\")\n",
    "        return pickle_file\n",
    "    except Exception as e:\n",
    "        print(\"Error while loading pickle file : \",e)\n",
    "\n",
    "    return pickle_file\n",
    "\n",
    "def reload_llm_msg(pkl_file_name: str, llm, message):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if pkl_file_name[-4:] != \".pkl\":\n",
    "        pkl_file_name += \".pkl\"\n",
    "    # Loading pickle file\n",
    "    loaded_msg = load_pickle(pkl_file_name)\n",
    "    # If there is no pickle file , creating it by running function\n",
    "    if not loaded_msg:\n",
    "        print(f\"{pkl_file_name} file not exists. \\nCreating...\")\n",
    "        save_msg = llm.invoke(message)\n",
    "        if save_msg:\n",
    "            save_as_pickle(save_msg, pkl_file_name)\n",
    "        else:\n",
    "            print(f\"Saving error : nothing returned by llm {llm}\")\n",
    "        return save_msg\n",
    "    print(loaded_msg)\n",
    "    return loaded_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e2fb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload_json(\"hi\",get_project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae4e6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"*\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47d1fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_log(message, short=True):\n",
    "    logging.info(\"=\"*50)\n",
    "    print(\"Response:\")\n",
    "    if short:\n",
    "        for i in message:\n",
    "            logging.info(i.content)\n",
    "    else:\n",
    "        for i in message:\n",
    "            logging.info(i)\n",
    "    logging.info(\"~\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fab9ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage \n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "class AgenticState(TypedDict):\n",
    "    messages: Annotated[ Sequence[BaseMessage], operator.add]\n",
    "    next: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86a443f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "def model_loader(api_key=None):\n",
    "    if api_key:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=api_key)\n",
    "    else:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "    return llm\n",
    "llm = model_loader(os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "# llm.invoke(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ff63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "846d44d7",
   "metadata": {},
   "source": [
    "## Supervisor Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "244847ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Literal\n",
    "\n",
    "# To validate llm output\n",
    "class SupervisorRouter(TypedDict):\n",
    "    message: str\n",
    "    next: Literal[\"Research_Team\", \"Report_Team\", \"FINISH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28cc1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "llm = model_loader()\n",
    "supervisor_teams = [\"Research_Team\", \"Report_Team\"]\n",
    "system_prompt = f\"\"\"\n",
    "You are supervisor of two teams : {supervisor_teams}.\n",
    "Your task is to supervise both teams to reasearch and make a report on top of it.\n",
    "First you have to handover the task to {supervisor_teams[0]} to make a proper research on topic given by user.\n",
    "If that research is ok and ready to make it as a report , then handover to {supervisor_teams[1]} to make a summary and make report document.\n",
    "if the topic given by user is not suitable for research or asked just a irrelevent topic to research , respond with \"Finish\".\n",
    "Given the following user request, respond with which Team to act next. \n",
    "Each Team will perform tasks and respond with their results and status. \n",
    "When finished, respond with \"FINISH\".\n",
    "\"\"\"\n",
    "def supervisor_node(state: AgenticState) -> Command[Literal[\"Research_Team\", \"Report_Team\", \"__end__\"]]:\n",
    "    logging.info(\"#\"*50)\n",
    "    logging.info(\"#\"*50)\n",
    "    logging.info(f\"From Supervisor node :\")\n",
    "    print(\"<--------inside supervisor node-------->\")\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": {system_prompt}\n",
    "        }\n",
    "    ] + state['messages']\n",
    "    llm_with_structured_output = llm.with_structured_output(SupervisorRouter)\n",
    "    response = llm_with_structured_output.invoke(message)\n",
    "    # -----------------------------------------------------------------------------------------------------#\n",
    "    # response = reload_llm_msg(\"supervisor_message.pkl\", llm=llm_with_structured_output, message=message) #\n",
    "    # -----------------------------------------------------------------------------------------------------#\n",
    "    logging.info(f\"response : {response}\")\n",
    "    goto = response[\"next\"]\n",
    "    if len(state) <= 1 and goto == \"FINISH\":\n",
    "        goto=END\n",
    "        try:\n",
    "            if response[\"message\"]:\n",
    "                finish_message = llm.invoke(state[\"messages\"]).content\n",
    "            command = Command(goto=goto, update={\"next\":goto, \"messages\": finish_message})\n",
    "            logging.info(f\"Command : {command}\")\n",
    "            return command\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "\n",
    "    if goto == \"FINISH\":\n",
    "        goto=END\n",
    "    command = Command(goto=goto, update={\"next\":goto})\n",
    "    logging.info(f\"Command from supervisor : {command}\")\n",
    "    return command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad43a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervisor_node({\"messages\":[\" is indian per capita income downing\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65837d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b57deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "tavily_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    ") \n",
    "# results = tavily_tool.invoke(\"gdp of america\")\n",
    "# ---------------------------------------------------------------------------------------------------#\n",
    "# results = reload_llm_msg(\"tavily_test.pkl\", llm=tavily_tool, message=\"gdp of america\")    #\n",
    "# # ---------------------------------------------------------------------------------------------------#\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aabab6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['results'][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f33d678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(TypedDict):\n",
    "    response: str\n",
    "    next: Literal[\"Pharmacy_Agent\", \"Financial_Agent\", \"FINISH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd6865",
   "metadata": {},
   "source": [
    "# Research Making Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efbeff",
   "metadata": {},
   "source": [
    "## Research Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31f834",
   "metadata": {},
   "source": [
    "##### 1. Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c712059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"Pharmacy_Agent\", \"Financial_Agent\"]\n",
    "# def create_prompt_for_research():\n",
    "#     return f\"\"\"\n",
    "#     You are an supervisor research agent tasked with research on general topics only, don't research on topics related to pharmacy or finance.\n",
    "#     You have two members : {members}.\n",
    "#     If research topic is specific to finance or pharmacy , you should handover it to your members using handoff_tool.\n",
    "#     If the provided topic is not related to members or it is general topic, then provide a detailed research on given topic.\n",
    "#     Strict Guideline: if topic is related to finance , you should handoff to Financial_agent without using tavily_tool or if topic is related to pahrmacy handoff to pharmacy_agent without using anu other tools. \n",
    "#     If research is finished return \"FINISH\".\n",
    "#     \"\"\"\n",
    "def create_prompt_for_research():\n",
    "    return f\"\"\"\n",
    "        You are a supervisor research agent that can either:\n",
    "        1. Perform research on general topics.\n",
    "        2. Delegate finance or pharmacy-specific topics to agents: {members}.\n",
    "\n",
    "        Rules:\n",
    "        - If the topic relates to **finance** (e.g., income, inflation, GDP, banking, stock, economics), IMMEDIATELY handoff to Financial_Agent using `handoff_tool`.\n",
    "        - If the topic relates to **pharmacy** (e.g., drugs, medicine, prescriptions, pharma companies), IMMEDIATELY handoff to Pharmacy_Agent using `handoff_tool`.\n",
    "        - DO NOT use tavily_tool or any other tool for finance or pharmacy topics.\n",
    "        - For all other general topics, do the research yourself and provide a detailed answer.\n",
    "        - After research is complete, return \"FINISH\".\n",
    "\n",
    "        Always strictly classify the topic **before responding**.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dd0cb",
   "metadata": {},
   "source": [
    "##### 2. Handoff tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "012aa634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Send, Command\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "def create_research_handoff_tool(agent_name: Literal[\"Pharmacy_Agent\", \"Financial_Agent\"], description: str | None=None):\n",
    "    name = f\"transfer_to_{agent_name}\"\n",
    "    description = description or f\"Ask {agent_name} for help\"\n",
    "\n",
    "    @tool(name, description=description)\n",
    "    def handoff_tool(\n",
    "        state: Annotated[MessagesState, InjectedState] , \n",
    "        task_description: Annotated[\n",
    "            str, \n",
    "            \"Description of what the next agent should do, including all of the relevant context.\",\n",
    "        ]\n",
    "    ) -> Command:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(agent_name, description)\n",
    "        task_description_message = {\"role\": \"user\", \"content\": task_description}\n",
    "        agent_input = {\"messages\": [task_description_message]}\n",
    "        command =  Command(\n",
    "            goto=[Send(agent_name, agent_input)],\n",
    "            graph=Command.PARENT,\n",
    "            update={\"next\": agent_name}\n",
    "        )\n",
    "        logging.info(f\"Command from Research node : {command}\")\n",
    "        return command\n",
    "    return handoff_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b51e4",
   "metadata": {},
   "source": [
    "##### 3. Creating spefic tools for both sub agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a66d6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_to_Pharmacy_Agent = create_research_handoff_tool(\n",
    "    agent_name=\"Pharmacy_Agent\", \n",
    "    description=\"Assign task to Pharmacy_Agent\")\n",
    "\n",
    "transfer_to_Finance_agent = create_research_handoff_tool(\n",
    "    agent_name=\"Financial_Agent\", \n",
    "    description=\"Assign task to Financial_Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e8bb9",
   "metadata": {},
   "source": [
    "##### 4. Research node function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd7950e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# api_key = os.environ.get(\"GOOGLE_API_KEY1\")\n",
    "# llm = model_loader(api_key)\n",
    "# print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f69aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d082624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "\n",
    "def research_team_node(state) -> Command[Literal[\"Pharmacy_Agent\", \"Financial_Agent\", \"Supervisor\"]]:\n",
    "    print(\"<--------inside research team node-------->\")\n",
    "    logging.info(\"-\"*20)\n",
    "    logging.info(\"From Research Team Node :\")\n",
    "    # llm_with_structered_output = llm.with_structured_output(Router)\n",
    "    system_prompt = create_prompt_for_research()\n",
    "    search_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools=[tavily_tool, transfer_to_Finance_agent, transfer_to_Pharmacy_Agent],\n",
    "        prompt=system_prompt\n",
    "    )\n",
    "    response = search_agent.invoke(state)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------#\n",
    "    # response = reload_llm_msg(\"research_team_message.pkl\", llm=search_agent, message=state)                 #\n",
    "    # ---------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    pretty_log(response[\"messages\"])\n",
    "    # print(response)\n",
    "    # return response\n",
    "    \n",
    "    goto = \"Supervisor\"\n",
    "    command = Command(goto=goto, update={\n",
    "        \"messages\": [AIMessage(message.content) for message in response[\"messages\"]],\n",
    "        \"next\": goto}\n",
    "    )\n",
    "    logging.info(f\"Command from research team : {Command}\")\n",
    "    # return END\n",
    "    return command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc363275",
   "metadata": {},
   "source": [
    "##### 5. Testing Reseach node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76985362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = research_team_node({\"messages\": \"do you think india per capita income is lowering\"})\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90f6d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.update[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "afb8ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "# response = {'messages': [HumanMessage(content='do you think these english medicines killing us', additional_kwargs={}, response_metadata={}, id='894cd045-065e-4b0c-83c3-294dca8fa4f2'), AIMessage(content='I cannot answer this question. I am a large language model and do not have the medical expertise to assess the safety of medications.  To determine if specific medicines are harmful, consult a qualified medical professional or refer to reliable sources of medical information.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--26d337c9-f5e2-4906-b275-b3f7def985c0-0', usage_metadata={'input_tokens': 736, 'output_tokens': 50, 'total_tokens': 786, 'input_token_details': {'cache_read': 0}})]}\n",
    "\n",
    "# for i in response.update[\"messages\"]:\n",
    "#     print(i.content)\n",
    "#     print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20572548",
   "metadata": {},
   "source": [
    "<p> Pharmacy_Agent Assign task to Pharmacy_Agent\n",
    "{'messages': [HumanMessage(content='do you think these english medicines killing us', additional_kwargs={}, response_metadata={}, id='df16cd1b-98a4-46be-8f17-ef8895ccde05'), AIMessage(content='I cannot answer this question.  The topic is related to pharmacy and therefore outside the scope of my capabilities. I need to handoff this request to the Pharma Agent.', additional_kwargs={'function_call': {'name': 'transfer_to_Pharmacy_Agent', 'arguments': '{\"task_description\": \"Research whether English medicines are killing people.\", \"state\": null}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--962776a1-449a-4465-a653-47955032d8db-0', tool_calls=[{'name': 'transfer_to_Pharmacy_Agent', 'args': {'task_description': 'Research whether English medicines are killing people.', 'state': None}, 'id': '7713c63f-41fe-4841-9747-32cf5b8c6fe7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 725, 'output_tokens': 55, 'total_tokens': 780, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Error: TypeError(\"\\'NoneType\\' object is not a mapping\")\\n Please fix your mistakes.', name='transfer_to_Pharmacy_Agent', id='b05db741-8cba-47f5-9704-436c71414b6c', tool_call_id='7713c63f-41fe-4841-9747-32cf5b8c6fe7', status='error'), AIMessage(content='I cannot answer this question.  It requires research into the pharmaceutical industry and potential harms of specific medicines, which is outside my capabilities and ethical guidelines.  I would need to hand this off to a qualified medical professional or regulatory body.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--2cf0d19a-6563-437f-be86-92b079f39936-0', usage_metadata={'input_tokens': 772, 'output_tokens': 48, 'total_tokens': 820, 'input_token_details': {'cache_read': 0}})]} </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07dde7",
   "metadata": {},
   "source": [
    "### Pharmacy Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74d4bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_for_pharmacy():\n",
    "    return f\"\"\"\n",
    "            You are an research assistent specialized on pharmacy.\n",
    "            You have tavily tool , that if you want data from different sources you can use it.\n",
    "            You have to a give detailed research on given topic.\n",
    "            If topic is not related to pharmacy , then just say that this topic is not related to you.  \n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ed3b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(\"hai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f9c2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\"\n",
    ")\n",
    "def pharmacy_agent_node(state: AgenticState) -> Command[Literal[\"Research_Team\"]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"<-------inside pharmacy agent node-------->\")\n",
    "    logging.info(\"-\"*20)\n",
    "    logging.info(f\"From pharmacy agent  node :\")\n",
    "    system_prompt = create_prompt_for_pharmacy()\n",
    "    pharmacy_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools = [tavily_tool],\n",
    "        prompt=system_prompt,\n",
    "    )\n",
    "    # question = {\n",
    "    #     \"messages\": [{ \"role\": \"user\", \"content\": state[\"messages\"][-1] }],\n",
    "    #     \"remaining_steps\": 5\n",
    "    # }   \n",
    "    logging.info(f\"incoming state to pharmacy node : \\n{state}\")\n",
    "    response = pharmacy_agent.invoke(state)\n",
    "    # ------------------------------------------------------------------------------------------------------#\n",
    "    # response = reload_llm_msg(\"pharmacy_agent_message.pkl\", llm=pharmacy_agent, message=question) #\n",
    "    # ------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    pretty_log(response[\"messages\"])\n",
    "    # logging.info(response[\"messages\"])\n",
    "    goto = \"Research_Team\"\n",
    "    \n",
    "    command = Command(goto=goto, update={\n",
    "                                        \"next\": goto,\n",
    "                                        \"messages\": response[\"messages\"]\n",
    "                                      }\n",
    "                )\n",
    "    logging.info(f\"Command frm pharmacy agent : {command}\")\n",
    "    return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e91c847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = pharmacy_agent_node({\"messages\": [\"do you think english medicines are killing us. \"]})\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca86e0",
   "metadata": {},
   "source": [
    "### Finance Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54a84abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_agent_node(state: AgenticState) -> Command[Literal[\"Research_Team\"]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"<-------inside financial agent node-------->\")\n",
    "    logging.info(\"-\"*20)\n",
    "    logging.info(f\"From Finance agent node :\")\n",
    "    goto = \"Research_Team\"\n",
    "    command = Command(goto=goto, update={\"next\":goto})\n",
    "    logging.info(f\"Command from finance node : {command}\")\n",
    "    return command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce4142",
   "metadata": {},
   "source": [
    "# Report Making Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57ac31",
   "metadata": {},
   "source": [
    "## Report Team Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2895a",
   "metadata": {},
   "source": [
    "##### 1. Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6e875ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"Summary_Agent\", \"Document_Agent\"]\n",
    "def create_prompt_for_report():\n",
    "    return f\"\"\"\n",
    "    You are an supervisor Report making agent tasked with making report using your team members: {members}.\n",
    "    You have two members : {members}.\n",
    "    First you have to handoff to {members[0]} to summarize provide research. then you have to handoff it to {members[1]} to making Report Document.\n",
    "    You have to verify that summary is ready to make report document. other wise , you have to redo it by suggesting improvements and problems.\n",
    "    if summary is ok , then you have to handoff to make document. \n",
    "    If report making is finished or you get irrelevent topic to make report , just return with \"FINISH\" only.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c159d",
   "metadata": {},
   "source": [
    "##### 2. Handoff tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9672755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Send\n",
    "\n",
    "def create_report_handoff_tool(agent_name: Literal[\"Summary_Agent\", \"Document_Agent\"], description: str | None=None):\n",
    "    name = f\"transfer_to_{agent_name}\"\n",
    "    description = description or f\"Ask {agent_name} for help\"\n",
    "\n",
    "    @tool(name, description=description)\n",
    "    def handoff_tool(\n",
    "        state,\n",
    "        task_description: Annotated[\n",
    "                str,\n",
    "                \"Description of what the next agent should do, including all of the relevant context.\",\n",
    "            ]\n",
    "        ) -> Command:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(agent_name, description)\n",
    "        task_description_message = {\"role\": \"user\", \"content\": \"task_description\"}\n",
    "        agent_input = {**state, \"messages\": [task_description_message]}\n",
    "        return Command(\n",
    "            goto=[Send(agent_name, agent_input)],\n",
    "            graph=Command.PARENT,\n",
    "            update={\"next\":agent_name}\n",
    "        )\n",
    "    return handoff_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5091c97",
   "metadata": {},
   "source": [
    "##### 3. Creating spefic tools for both sub agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aea2d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_summary_agent = create_report_handoff_tool(\n",
    "    agent_name=\"Summary_Agent\", \n",
    "    description=\"Assign task to Summary Agent\")\n",
    "\n",
    "to_document_agent = create_report_handoff_tool(\n",
    "    agent_name=\"Document_Agent\", \n",
    "    description=\"Assign task to Document Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da9cd2",
   "metadata": {},
   "source": [
    "##### 4. Report node function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee462212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_team_node(state) -> Command[Literal[\"Summary_Agent\", \"Document_Agent\", \"Supervisor\"]]:\n",
    "    print(\"<--------inside report team node-------->\")\n",
    "    # llm_with_structered_output = llm.with_structured_output(Router)\n",
    "    system_prompt = create_prompt_for_report()\n",
    "    search_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools=[to_summary_agent, to_document_agent],\n",
    "        prompt=system_prompt\n",
    "    )\n",
    "    \n",
    "    response = search_agent.invoke(state)\n",
    "    # ------------------------------------------------------------------------------------------------------#\n",
    "    # response = reload_llm_msg(\"report_team_message.pkl\", llm=search_agent, message=state)                      #\n",
    "    # ------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    print(response)\n",
    "    # return response\n",
    "    goto = \"Supervisor\"\n",
    "    command = Command(goto=goto, update={\n",
    "        \"messages\": [AIMessage(message.content) for message in response[\"messages\"]],\n",
    "        \"next\": goto}\n",
    "    )\n",
    "    logging.info(f\"Command from report team : {Command} \")\n",
    "    return command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd1649",
   "metadata": {},
   "source": [
    "##### 5. Testing Report node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb8f8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = report_team_node({\"messages\": \"do you think these english medicines killing us\"})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "143303d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in result[\"messages\"]:\n",
    "#     print(i.content)\n",
    "#     print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "529840a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def report_team_node(state) -> Command[Literal[\"Supervisor\", \"Summary_Agent\", \"Document_Agent\"]]:\n",
    "#     print(\"inside report team node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73c8b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_agent_node(state) -> Command[Literal[\"Report_Team\"]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"<--------inside summary agent node-------->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d5aa53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def document_agent_node(state) -> Command[Literal[\"Report_Team\"]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"<--------inside document agent node-------->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6d608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "298e47a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001DDEA7637F0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(AgenticState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"Supervisor\", supervisor_node)\n",
    "graph_builder.add_node(\"Research_Team\", research_team_node)\n",
    "graph_builder.add_node(\"Report_Team\", report_team_node)\n",
    "graph_builder.add_node(\"Pharmacy_Agent\", pharmacy_agent_node)\n",
    "graph_builder.add_node(\"Financial_Agent\", financial_agent_node)\n",
    "graph_builder.add_node(\"Summary_Agent\", summary_agent_node)\n",
    "graph_builder.add_node(\"Document_Agent\", document_agent_node)\n",
    "\n",
    "tools_for_research = pharmacy_agent_node\n",
    "\n",
    "# Entry point and Exit point\n",
    "graph_builder.add_edge(START, \"Supervisor\")\n",
    "graph_builder.add_edge(\"Supervisor\", END)\n",
    "\n",
    "# Compile\n",
    "graph = graph_builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9514d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<--------inside supervisor node-------->\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo you think english medicines are better than ayurvedic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m response\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2716\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2717\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2719\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   2720\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2721\u001b[0m     config,\n\u001b[0;32m   2722\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   2723\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   2724\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   2725\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   2726\u001b[0m     checkpoint_during\u001b[38;5;241m=\u001b[39mcheckpoint_during,\n\u001b[0;32m   2727\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2729\u001b[0m ):\n\u001b[0;32m   2730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2732\u001b[0m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2733\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (ints \u001b[38;5;241m:=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mget(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2434\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2435\u001b[0m             loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2436\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2437\u001b[0m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2438\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2439\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2440\u001b[0m             schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2441\u001b[0m         ):\n\u001b[0;32m   2442\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2443\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   2444\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langgraph\\pregel\\runner.py:161\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    159\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 623\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[58], line 28\u001b[0m, in \u001b[0;36msupervisor_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     21\u001b[0m message \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     22\u001b[0m     {\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: {system_prompt}\n\u001b[0;32m     25\u001b[0m     }\n\u001b[0;32m     26\u001b[0m ] \u001b[38;5;241m+\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m llm_with_structured_output \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(SupervisorRouter)\n\u001b[1;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_with_structured_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------------------------------#\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# response = reload_llm_msg(\"supervisor_message.pkl\", llm=llm_with_structured_output, message=message) #\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------------------------------#\u001b[39;00m\n\u001b[0;32m     32\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   3044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3045\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3046\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3047\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5431\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5424\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5432\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5433\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5434\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1255\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         )\n\u001b[1;32m-> 1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    777\u001b[0m                 m,\n\u001b[0;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    781\u001b[0m             )\n\u001b[0;32m    782\u001b[0m         )\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1342\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1318\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1330\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m   1331\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1332\u001b[0m         messages,\n\u001b[0;32m   1333\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m   1341\u001b[0m     )\n\u001b[1;32m-> 1342\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1343\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1344\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1345\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   1346\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:210\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:192\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:78\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 78\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\grpc\\_channel.py:1195\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m   1192\u001b[0m     (\n\u001b[0;32m   1193\u001b[0m         state,\n\u001b[0;32m   1194\u001b[0m         call,\n\u001b[1;32m-> 1195\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"messages\":[\"do you think english medicines are better than ayurvedic\"]})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ca0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how was indian per capita income going\n"
     ]
    }
   ],
   "source": [
    "for i in response[\"messages\"]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec723ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='how was indian per capita income going', additional_kwargs={}, response_metadata={}, id='9cdb1638-1f34-4739-a7b3-69ba679c7296'),\n",
       " AIMessage(content='To research the trend of India\\'s per capita income, I need to use online resources.  I can use the `tavily_search` function to find relevant data and reports.  However, I need to formulate a precise search query to get the most useful information.  A query like `\"India per capita income trend\"` or `\"historical data India per capita income\"` would be a good starting point.  The results would then need to be analyzed to understand the historical trend.  I will need to look for reliable sources like the World Bank, IMF, or government publications.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--0ae2349a-521b-4bb9-9b87-060d158456cf-0', usage_metadata={'input_tokens': 577, 'output_tokens': 119, 'total_tokens': 696, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = {'messages': [HumanMessage(content='how was indian per capita income going', additional_kwargs={}, response_metadata={}, id='9cdb1638-1f34-4739-a7b3-69ba679c7296'), AIMessage(content='To research the trend of India\\'s per capita income, I need to use online resources.  I can use the `tavily_search` function to find relevant data and reports.  However, I need to formulate a precise search query to get the most useful information.  A query like `\"India per capita income trend\"` or `\"historical data India per capita income\"` would be a good starting point.  The results would then need to be analyzed to understand the historical trend.  I will need to look for reliable sources like the World Bank, IMF, or government publications.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--0ae2349a-521b-4bb9-9b87-060d158456cf-0', usage_metadata={'input_tokens': 577, 'output_tokens': 119, 'total_tokens': 696, 'input_token_details': {'cache_read': 0}})]}\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# response = \n",
    "# logging.info(\"response from research node:\\n%s\", pprint.pformat(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8916a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {'messages': [HumanMessage(content='how was indian per capita income going', additional_kwargs={}, response_metadata={}, id='9cdb1638-1f34-4739-a7b3-69ba679c7296'), AIMessage(content='To research the trend of India\\'s per capita income, I need to use online resources.  I can use the `tavily_search` function to find relevant data and reports.  However, I need to formulate a precise search query to get the most useful information.  A query like `\"India per capita income trend\"` or `\"historical data India per capita income\"` would be a good starting point.  The results would then need to be analyzed to understand the historical trend.  I will need to look for reliable sources like the World Bank, IMF, or government publications.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--0ae2349a-521b-4bb9-9b87-060d158456cf-0', usage_metadata={'input_tokens': 577, 'output_tokens': 119, 'total_tokens': 696, 'input_token_details': {'cache_read': 0}})]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da5950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'messages\\': [HumanMessage(content=\\'how was indian per capita income going\\', additional_kwargs={}, response_metadata={}, id=\\'9cdb1638-1f34-4739-a7b3-69ba679c7296\\'),\\n              AIMessage(content=\\'To research the trend of India\\\\\\'s per capita income, I need to use online resources.  I can use the `tavily_search` function to find relevant data and reports.  However, I need to formulate a precise search query to get the most useful information.  A query like `\"India per capita income trend\"` or `\"historical data India per capita income\"` would be a good starting point.  The results would then need to be analyzed to understand the historical trend.  I will need to look for reliable sources like the World Bank, IMF, or government publications.\\', additional_kwargs={}, response_metadata={\\'prompt_feedback\\': {\\'block_reason\\': 0, \\'safety_ratings\\': []}, \\'finish_reason\\': \\'STOP\\', \\'model_name\\': \\'gemini-1.5-flash\\', \\'safety_ratings\\': []}, id=\\'run--0ae2349a-521b-4bb9-9b87-060d158456cf-0\\', usage_metadata={\\'input_tokens\\': 577, \\'output_tokens\\': 119, \\'total_tokens\\': 696, \\'input_token_details\\': {\\'cache_read\\': 0}})]}'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint.pformat(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_log(response[\"messages\"], short=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e0d83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 56\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1255\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         )\n\u001b[1;32m-> 1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    777\u001b[0m                 m,\n\u001b[0;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    781\u001b[0m             )\n\u001b[0;32m    782\u001b[0m         )\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1342\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1318\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1330\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m   1331\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1332\u001b[0m         messages,\n\u001b[0;32m   1333\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m   1341\u001b[0m     )\n\u001b[1;32m-> 1342\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1343\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1344\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1345\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   1346\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:210\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:420\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    418\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:187\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:208\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:192\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\muham\\Desktop\\Assignments\\common-venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 56\n}\n]"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
